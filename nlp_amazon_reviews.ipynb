{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, json\n",
    "\n",
    "class Sentiment:\n",
    "    NEGATIVE = \"NEGATIVE\"\n",
    "    NEUTRAL = \"NEUTRAL\"\n",
    "    POSITIVE = \"POSITIVE\"\n",
    "\n",
    "class Review:\n",
    "    def __init__(self, text, score):\n",
    "        self.text = text\n",
    "        self.score = score\n",
    "        self.sentiment = self.get_sentiment()\n",
    "        \n",
    "    def get_sentiment(self):\n",
    "        if self.score <= 2:\n",
    "            return Sentiment.NEGATIVE\n",
    "        elif self.score == 3:\n",
    "            return Sentiment.NEUTRAL\n",
    "        else: #score of 4 or 5\n",
    "            return Sentiment.POSITIVE\n",
    "        \n",
    "class ReviewContainer:\n",
    "    def __init__(self, reviews):\n",
    "        self.reviews = reviews\n",
    "        \n",
    "    def get_text(self):\n",
    "        return [x.text for x in self.reviews]\n",
    "    \n",
    "    def get_sentiment(self):\n",
    "        return [x.sentiment for x in self.reviews]\n",
    "        \n",
    "    def evenly_distribute(self):\n",
    "        #creates container for x.sentiment where sentiment is NEGATIVE and POSITIVE\n",
    "        negative = list(filter(lambda x: x.sentiment == Sentiment.NEGATIVE, self.reviews))\n",
    "        positive = list(filter(lambda x: x.sentiment == Sentiment.POSITIVE, self.reviews))\n",
    "        #makes positive = to len of negative\n",
    "        positive_shrunk = positive[:len(negative)] \n",
    "        self.reviews = negative + positive_shrunk\n",
    "        #if not shuffle, all pos will appear first, then neg\n",
    "        random.shuffle(self.reviews)\n",
    "        #print(negative[0].text)\n",
    "        #print(len(negative))\n",
    "        #print(len(positive))\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I hoped for Mia to have some peace in this book, but her story is so real and raw.  Broken World was so touching and emotional because you go from Mia\\'s trauma to her trying to cope.  I love the way the story displays how there is no \"just bouncing back\" from being sexually assaulted.  Mia showed us how those demons come for you every day and how sometimes they best you. I was so in the moment with Broken World and hurt with Mia because she was surrounded by people but so alone and I understood her feelings.  I found myself wishing I could give her some of my courage and strength or even just to be there for her.  Thank you Lizzy for putting a great character\\'s voice on a strong subject and making it so that other peoples story may be heard through Mia\\'s.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "file_name = \"./data/sentiment/books_small_10000.json\"\n",
    "\n",
    "reviews = []\n",
    "\n",
    "with open(file_name) as f:\n",
    "    for line in f:\n",
    "        review = json.loads(line)\n",
    "        reviews.append(Review(review[\"reviewText\"], review[\"overall\"]))\n",
    "        \n",
    "reviews[5].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training, test = train_test_split(reviews, test_size=0.33, random_state=42)\n",
    "\n",
    "#train_container = ReviewContainer(training)\n",
    "#test_container = ReviewContainer(test)\n",
    "\n",
    "import sys\n",
    "sys.path.append('./venv/lib/python3.8/site-packages')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "training, test = train_test_split(reviews, test_size=0.33, random_state=42)\n",
    "\n",
    "train_container = ReviewContainer(training)\n",
    "\n",
    "test_container = ReviewContainer(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "436"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_container.evenly_distribute()\n",
    "train_x = train_container.get_text()\n",
    "train_y = train_container.get_sentiment()\n",
    "\n",
    "test_container.evenly_distribute()\n",
    "test_x = test_container.get_text()\n",
    "test_y = test_container.get_sentiment()\n",
    "\n",
    "train_y.count(Sentiment.POSITIVE)\n",
    "train_y.count(Sentiment.NEGATIVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of words vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This would have to have been the worst of Lynda La Plantes books that I've read. It took ages to get into the story and I felt that there was a lot of padding. If I could have given the story zero stars I would have. It's one of those books that you'd love to have your money back for having bought it as it promises more and delivers little. A very weak story! Don't waste your money!\n",
      "  (0, 8614)\t0.13044101240659642\n",
      "  (0, 2383)\t0.09274844476658857\n",
      "  (0, 8634)\t0.1529832650486529\n",
      "  (0, 8497)\t0.07267585752376987\n",
      "  (0, 4722)\t0.09517294430585763\n",
      "  (0, 2096)\t0.19606481431716546\n",
      "  (0, 5185)\t0.07166513032361159\n",
      "  (0, 6187)\t0.20797753575303438\n",
      "  (0, 569)\t0.06947767307619718\n",
      "  (0, 1022)\t0.14342222927061166\n",
      "  (0, 3671)\t0.12305729746526922\n",
      "  (0, 3177)\t0.05073787731272773\n",
      "  (0, 698)\t0.10228711252975144\n",
      "  (0, 5168)\t0.24611459493053844\n",
      "  (0, 8883)\t0.18101771067661412\n",
      "  (0, 4782)\t0.08400677012341512\n",
      "  (0, 8879)\t0.06463043055276674\n",
      "  (0, 7984)\t0.11340572113445474\n",
      "  (0, 5514)\t0.06635580213104168\n",
      "  (0, 7457)\t0.13622043680603432\n",
      "  (0, 8897)\t0.19606481431716546\n",
      "  (0, 3394)\t0.12941001588095422\n",
      "  (0, 1790)\t0.08488386170768662\n",
      "  (0, 3968)\t0.075125641390007\n",
      "  (0, 5652)\t0.19606481431716546\n",
      "  :\t:\n",
      "  (0, 8608)\t0.05073787731272773\n",
      "  (0, 7951)\t0.07609966604182748\n",
      "  (0, 3017)\t0.11522364438724637\n",
      "  (0, 423)\t0.07431227150989422\n",
      "  (0, 7533)\t0.18467174675719966\n",
      "  (0, 4212)\t0.08422360052545293\n",
      "  (0, 3374)\t0.080738918017033\n",
      "  (0, 297)\t0.18761260394769194\n",
      "  (0, 8080)\t0.13150950783474277\n",
      "  (0, 4277)\t0.17096691942124492\n",
      "  (0, 6392)\t0.05717233484734902\n",
      "  (0, 8467)\t0.10661556953485855\n",
      "  (0, 7925)\t0.15671269924226564\n",
      "  (0, 996)\t0.16631021019461317\n",
      "  (0, 5913)\t0.20797753575303438\n",
      "  (0, 4515)\t0.20797753575303438\n",
      "  (0, 4824)\t0.20797753575303438\n",
      "  (0, 5478)\t0.1282251895659337\n",
      "  (0, 8816)\t0.14510156553944242\n",
      "  (0, 7929)\t0.1053697568687175\n",
      "  (0, 805)\t0.08997465178302026\n",
      "  (0, 8052)\t0.11454730290068237\n",
      "  (0, 3669)\t0.29899879216218367\n",
      "  (0, 8823)\t0.15701284546013802\n",
      "  (0, 7976)\t0.03938747708240024\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# This book is great\n",
    "# This book was so bad\n",
    "# Count vectorizer (term-frequency) weighs word appearance equally\n",
    "# 'This' appears in both, and weighs same as 'great'\n",
    "\n",
    "# Term-frequency inverse-document frequency\n",
    "# Gives less weight to words that appear more frequently\n",
    "# Indicating there's more meaning to be inferred \n",
    "# from words that appear less often\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_x_vectors = vectorizer.fit_transform(train_x)\n",
    "\n",
    "test_x_vectors = vectorizer.transform(test_x)\n",
    "\n",
    "\n",
    "#vectorizer = CountVectorizer()\n",
    "#train_x_vectors = vectorizer.fit_transform(train_x)\n",
    "\n",
    "#test_x_vectors = vectorizer.transform(test_x)\n",
    "\n",
    "\n",
    "print(train_x[0])\n",
    "print(train_x_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['POSITIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "# various algos are fit to the training data(svm, dec, gnb, log)\n",
    "clf_svm = svm.SVC(kernel='linear')\n",
    "\n",
    "# here we fit the model to our training data \n",
    "clf_svm.fit(train_x_vectors, train_y)\n",
    "\n",
    "test_x[0]\n",
    "test_x_vectors[0]\n",
    "# here we make a prediction on the first test sample\n",
    "clf_svm.predict(test_x_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['POSITIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf_dec = DecisionTreeClassifier()\n",
    "clf_dec.fit(train_x_vectors, train_y)\n",
    "\n",
    "clf_dec.predict(test_x_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NEGATIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf_gnb = GaussianNB()\n",
    "clf_gnb.fit(train_x_vectors.toarray(), train_y)\n",
    "\n",
    "clf_gnb.predict(test_x_vectors[0].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['POSITIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn import preprocessing\n",
    "\n",
    "\n",
    "clf_lr = LogisticRegression(max_iter=10000)\n",
    "#train_x_pre = preprocessing.scale(train_x_vectors)\n",
    "clf_lr.fit(train_x_vectors, train_y)\n",
    "\n",
    "clf_dec.predict(test_x_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8076923076923077\n",
      "0.6466346153846154\n",
      "0.6610576923076923\n",
      "0.8052884615384616\n"
     ]
    }
   ],
   "source": [
    "# here we get the accuracy score of the classification \n",
    "print(clf_svm.score(test_x_vectors, test_y))\n",
    "print(clf_dec.score(test_x_vectors, test_y))\n",
    "print(clf_gnb.score(test_x_vectors.toarray(), test_y))\n",
    "print(clf_lr.score(test_x_vectors, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8076923076923077\n",
      "0.6466346153846154\n",
      "0.6610576923076923\n",
      "0.8052884615384616\n"
     ]
    }
   ],
   "source": [
    "# here we get the accuracy score of the classification \n",
    "print(clf_svm.score(test_x_vectors, test_y))\n",
    "print(clf_dec.score(test_x_vectors, test_y))\n",
    "print(clf_gnb.score(test_x_vectors.toarray(), test_y))\n",
    "print(clf_lr.score(test_x_vectors, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8076923076923077"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svm.score(test_x_vectors, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6466346153846154"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_dec.score(test_x_vectors, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6610576923076923"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_gnb.score(test_x_vectors.toarray(), test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8052884615384616"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_lr.score(test_x_vectors, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.80952381 0.         0.80582524]\n",
      "[0.65411765 0.         0.63882064]\n",
      "[0.66508314 0.         0.65693431]\n",
      "[0.80760095 0.         0.80291971]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1464: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "./venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1464: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(test_y, clf_svm.predict(test_x_vectors), average=None, labels = [Sentiment.NEGATIVE, Sentiment.NEUTRAL, Sentiment.POSITIVE]))\n",
    "print(f1_score(test_y, clf_dec.predict(test_x_vectors), average=None, labels = [Sentiment.NEGATIVE, Sentiment.NEUTRAL, Sentiment.POSITIVE]))\n",
    "print(f1_score(test_y, clf_gnb.predict(test_x_vectors.toarray()), average=None, labels = [Sentiment.NEGATIVE, Sentiment.NEUTRAL, Sentiment.POSITIVE]))\n",
    "print(f1_score(test_y, clf_lr.predict(test_x_vectors), average=None, labels = [Sentiment.NEGATIVE, Sentiment.NEUTRAL, Sentiment.POSITIVE]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['POSITIVE', 'NEGATIVE', 'NEGATIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = ['Amazing book it was lit', 'horrible book do not buy', 'bad book do not buy']\n",
    "new_test = vectorizer.transform(test_set)\n",
    "\n",
    "clf_svm.predict(new_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.80952381, 0.        , 0.80582524])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test_y, clf_svm.predict(test_x_vectors), average=None, labels = [Sentiment.NEGATIVE, Sentiment.NEUTRAL, Sentiment.POSITIVE])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.65411765, 0.        , 0.63882064])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test_y, clf_dec.predict(test_x_vectors), average=None, labels = [Sentiment.NEGATIVE, Sentiment.NEUTRAL, Sentiment.POSITIVE])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1464: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.80952381, 0.        , 0.80582524])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test_y, clf_svm.predict(test_x_vectors.toarray()), average=None, labels = [Sentiment.NEGATIVE, Sentiment.NEUTRAL, Sentiment.POSITIVE])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.80952381, 0.        , 0.80582524])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test_y, clf_svm.predict(test_x_vectors), average=None, labels = [Sentiment.NEGATIVE, Sentiment.NEUTRAL, Sentiment.POSITIVE])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Olivia Hampton arrives at the Dunraven family home as cataloger of their extensive library. What she doesn't expect is a broken carriage wheel on the way. Nor a young girl whose mind is clearly gone, an old man in need of care himself (and doesn&#8217;t quite seem all there in Olivia&#8217;s opinion). Furthermore, Marion Dunraven, the only sane one of the bunch and the one Olivia is inexplicable drawn to, seems captive to everyone in the dusty old house. More importantly, she doesn't expect to fall in love with Dunraven's daughter Marion.Can Olivia truly believe the stories of sadness and death that surround the house, or are they all just local neighborhood rumor?Was that carriage trouble just a coincidence or a supernatural sign to stay away? If she remains, will the Castle&#8217;s dark shadows take Olivia down with them or will she and Marion long enough to declare their love?Patty G. Henderson has created an atmospheric and intriguing story in her Gothic tale. I found this to be an enjoyable read, even if it isn&#8217;t my usual preferred genre. I think, with this tale, I got hooked on the old Gothic romantic style. So I think fans of the genre (and of lesbian romances) will enjoy it.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2767"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following are checks on the data\n",
    "\n",
    "train_y.count(Sentiment.POSITIVE)\n",
    "test_y.count(Sentiment.POSITIVE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.count(Sentiment.NEUTRAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.count(Sentiment.NEGATIVE)\n",
    "test_y.count(Sentiment.NEGATIVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning our model (with Grid Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=SVC(),\n",
       "             param_grid={'C': (1, 4, 8, 16, 32), 'kernel': ('rbf', 'linear')})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'kernel': ('rbf', 'linear'), 'C': (1,4,8,16, 32)}\n",
    "\n",
    "svc = svm.SVC()\n",
    "clf = GridSearchCV(svc, parameters, cv=3)\n",
    "clf.fit(train_x_vectors, train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
